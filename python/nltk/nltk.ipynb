{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import pos_tag\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dhrions/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dhrions/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/dhrions/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dhrions/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les ressources nécessaires\n",
    "nltk.download('punkt')  # Tokenizer pour les phrases et les mots\n",
    "nltk.download('averaged_perceptron_tagger')  # Tagger pour le Part-of-Speech (POS) tagging\n",
    "nltk.download('wordnet')  # Base de données lexicales pour le WordNet\n",
    "nltk.download('stopwords')  # Liste de mots vides (stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "demain_des_aube = \"\"\"\n",
    "Demain, dès l’aube, à l’heure où blanchit la campagne,\n",
    "Je partirai. Vois-tu, je sais que tu m’attends.\n",
    "J’irai par la forêt, j’irai par la montagne.\n",
    "Je ne puis demeurer loin de toi plus longtemps.\n",
    "\n",
    "Je marcherai les yeux fixés sur mes pensées,\n",
    "Sans rien voir au dehors, sans entendre aucun bruit,\n",
    "Seul, inconnu, le dos courbé, les mains croisées,\n",
    "Triste, et le jour pour moi sera comme la nuit.\n",
    "\n",
    "Je ne regarderai ni l’or du soir qui tombe,\n",
    "Ni les voiles au loin descendant vers Harfleur,\n",
    "Et quand j’arriverai, je mettrai sur ta tombe\n",
    "Un bouquet de houx vert et de bruyère en fleur.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(demain_des_aube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Demain', ',', 'dès', 'l', '’', 'aube', ',', 'à', 'l', '’', 'heure', 'où', 'blanchit', 'la', 'campagne', ',', 'Je', 'partirai', '.', 'Vois-tu', ',', 'je', 'sais', 'que', 'tu', 'm', '’', 'attends', '.', 'J', '’', 'irai', 'par', 'la', 'forêt', ',', 'j', '’', 'irai', 'par', 'la', 'montagne', '.', 'Je', 'ne', 'puis', 'demeurer', 'loin', 'de', 'toi', 'plus', 'longtemps', '.', 'Je', 'marcherai', 'les', 'yeux', 'fixés', 'sur', 'mes', 'pensées', ',', 'Sans', 'rien', 'voir', 'au', 'dehors', ',', 'sans', 'entendre', 'aucun', 'bruit', ',', 'Seul', ',', 'inconnu', ',', 'le', 'dos', 'courbé', ',', 'les', 'mains', 'croisées', ',', 'Triste', ',', 'et', 'le', 'jour', 'pour', 'moi', 'sera', 'comme', 'la', 'nuit', '.', 'Je', 'ne', 'regarderai', 'ni', 'l', '’', 'or', 'du', 'soir', 'qui', 'tombe', ',', 'Ni', 'les', 'voiles', 'au', 'loin', 'descendant', 'vers', 'Harfleur', ',', 'Et', 'quand', 'j', '’', 'arriverai', ',', 'je', 'mettrai', 'sur', 'ta', 'tombe', 'Un', 'bouquet', 'de', 'houx', 'vert', 'et', 'de', 'bruyère', 'en', 'fleur', '.']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(demain_des_aube)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = []\n",
    "for word in words:\n",
    "    if word.casefold() not in stop_words:\n",
    "         filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Demain',\n",
       " ',',\n",
       " 'dès',\n",
       " '’',\n",
       " 'aube',\n",
       " ',',\n",
       " '’',\n",
       " 'heure',\n",
       " 'où',\n",
       " 'blanchit',\n",
       " 'campagne',\n",
       " ',',\n",
       " 'partirai',\n",
       " '.',\n",
       " 'Vois-tu',\n",
       " ',',\n",
       " 'sais',\n",
       " '’',\n",
       " 'attends',\n",
       " '.',\n",
       " '’',\n",
       " 'irai',\n",
       " 'forêt',\n",
       " ',',\n",
       " '’',\n",
       " 'irai',\n",
       " 'montagne',\n",
       " '.',\n",
       " 'puis',\n",
       " 'demeurer',\n",
       " 'loin',\n",
       " 'plus',\n",
       " 'longtemps',\n",
       " '.',\n",
       " 'marcherai',\n",
       " 'yeux',\n",
       " 'fixés',\n",
       " 'pensées',\n",
       " ',',\n",
       " 'Sans',\n",
       " 'rien',\n",
       " 'voir',\n",
       " 'dehors',\n",
       " ',',\n",
       " 'sans',\n",
       " 'entendre',\n",
       " 'aucun',\n",
       " 'bruit',\n",
       " ',',\n",
       " 'Seul',\n",
       " ',',\n",
       " 'inconnu',\n",
       " ',',\n",
       " 'dos',\n",
       " 'courbé',\n",
       " ',',\n",
       " 'mains',\n",
       " 'croisées',\n",
       " ',',\n",
       " 'Triste',\n",
       " ',',\n",
       " 'jour',\n",
       " 'comme',\n",
       " 'nuit',\n",
       " '.',\n",
       " 'regarderai',\n",
       " 'ni',\n",
       " '’',\n",
       " 'or',\n",
       " 'soir',\n",
       " 'tombe',\n",
       " ',',\n",
       " 'Ni',\n",
       " 'voiles',\n",
       " 'loin',\n",
       " 'descendant',\n",
       " 'vers',\n",
       " 'Harfleur',\n",
       " ',',\n",
       " 'quand',\n",
       " '’',\n",
       " 'arriverai',\n",
       " ',',\n",
       " 'mettrai',\n",
       " 'tombe',\n",
       " 'bouquet',\n",
       " 'houx',\n",
       " 'vert',\n",
       " 'bruyère',\n",
       " 'fleur',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "En français, on parle de « racinisation »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = FrenchStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demain',\n",
       " ',',\n",
       " 'des',\n",
       " '’',\n",
       " 'aub',\n",
       " ',',\n",
       " '’',\n",
       " 'heur',\n",
       " 'où',\n",
       " 'blanch',\n",
       " 'campagn',\n",
       " ',',\n",
       " 'part',\n",
       " '.',\n",
       " 'vois-tu',\n",
       " ',',\n",
       " 'sais',\n",
       " '’',\n",
       " 'attend',\n",
       " '.',\n",
       " '’',\n",
       " 'irai',\n",
       " 'forêt',\n",
       " ',',\n",
       " '’',\n",
       " 'irai',\n",
       " 'montagn',\n",
       " '.',\n",
       " 'puis',\n",
       " 'demeur',\n",
       " 'loin',\n",
       " 'plus',\n",
       " 'longtemp',\n",
       " '.',\n",
       " 'march',\n",
       " 'yeux',\n",
       " 'fix',\n",
       " 'pens',\n",
       " ',',\n",
       " 'san',\n",
       " 'rien',\n",
       " 'voir',\n",
       " 'dehor',\n",
       " ',',\n",
       " 'san',\n",
       " 'entendr',\n",
       " 'aucun',\n",
       " 'bruit',\n",
       " ',',\n",
       " 'seul',\n",
       " ',',\n",
       " 'inconnu',\n",
       " ',',\n",
       " 'dos',\n",
       " 'courb',\n",
       " ',',\n",
       " 'main',\n",
       " 'crois',\n",
       " ',',\n",
       " 'trist',\n",
       " ',',\n",
       " 'jour',\n",
       " 'comm',\n",
       " 'nuit',\n",
       " '.',\n",
       " 'regard',\n",
       " 'ni',\n",
       " '’',\n",
       " 'or',\n",
       " 'soir',\n",
       " 'tomb',\n",
       " ',',\n",
       " 'ni',\n",
       " 'voil',\n",
       " 'loin',\n",
       " 'descend',\n",
       " 'ver',\n",
       " 'harfleur',\n",
       " ',',\n",
       " 'quand',\n",
       " '’',\n",
       " 'arriv',\n",
       " ',',\n",
       " 'mettr',\n",
       " 'tomb',\n",
       " 'bouquet',\n",
       " 'houx',\n",
       " 'vert',\n",
       " 'bruyer',\n",
       " 'fleur',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in filtered_list]\n",
    "stemmed_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
